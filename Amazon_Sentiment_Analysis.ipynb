{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oB7Y8yFYHYWq"
   },
   "source": [
    "\n",
    "# Amazon Review Sentiment Analyzer \n",
    "\n",
    "## ***Abstract:*** \n",
    "\n",
    "\n",
    ">   Recently, text mining and computational linguistics research have grown extremely interested in the application challenge of sentiment analysis of product reviews. Sentiment analysis allows large-scale processing of data in an efficient and cost-effective manner. Here, we're interested in examining the relationship between customer ratings of products on Amazon and product reviews on Amazon.com as it influence the trends a lot.\n",
    "\n",
    "\n",
    "\n",
    "> Businesses are being compelled to employ creative strategies to improve customer experiences\n",
    "due to the fierce competition to acquire and retain customers online. Companies are\n",
    "increasingly looking at consumer reviews on internet platforms like Amazon to better\n",
    "understand how customers rank their products and services.\n",
    "\n",
    "\n",
    "\n",
    "> A study on amazon last year revealed that online shoppers trust reviews as much as personal recommendations. Any online item with large amount of positive reviews provides a powerful comment of the legitimacy of the item. Conversely, books, or any other online item, without reviews puts potential prospects in a state of distrust. Quite simply, more reviews look more convincing. People value the consent and experience of others and the review on a material is the only way to understand others impression on the product. Opinions, collected from users' experiences regarding specific products or topics, straightforwardly influence future customer purchase decisions.\n",
    "\n",
    "\n",
    "\n",
    "> The word \"sentiment analysis\" refers to as opinion mining, is a ***natural language processing (NLP)*** approach for detecting the emotional tone of a body of text; the process of determining if a text contains negative, positive, or neutral emotions. Natural\n",
    "language processing (NLP) and machine learning are used in this type of text analytics.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "> > Thus, such kind of analysis problems will require ***ML*** to come into action. ***Machine Learning algorithm like Naive Bayes' and Neural Networks can be used in solving such kind of problems.***\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tJEzF8SRCrb5"
   },
   "source": [
    "## ***Importance:***\n",
    "\n",
    "> > The goal of this NLP based project is to classify customer reviews of various items into various feedback classes, and to develop a supervised learning model to polarise a huge number of reviews along with choosing the optimal number of classes in which the data can be classified.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qEgDuHDRHh97"
   },
   "source": [
    "## ***1. Importing the required libraries such as pandas, numpy, re, sklearn, nltk etc.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "z4iIZP8NldFA"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ca_aZnNwFXB0"
   },
   "source": [
    "## ***2. Converting file into .csv:***\n",
    "\n",
    "\n",
    "\n",
    "### > The dataset was unlabeled and to use it in a supervised learning model we had to label the data. ***We used JSON files for AMAZON_FASHION department where the structure of the data is as follows:***\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "“reviewerID”: ID of the reviewer\n",
    "\n",
    "“asin”: ID of the product\n",
    "\n",
    "“reviewerName”: name of the reviewer\n",
    "\n",
    "“reviewText”: text of the review\n",
    "\n",
    "“overall”: rating of the product\n",
    "\n",
    "“summary”: summary of the review\n",
    "\n",
    "“reviewTime”: time of the review (raw)\n",
    "\n",
    "'unixReviewTime' : unix time of review \n",
    "\n",
    "'style': colour\n",
    "\n",
    "'image': link for the image\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "> The initial file format of the dataset is .json. We need to convert it to .csv format to make it convenient to access and manipulate data.\n",
    "\n",
    "***P.S : I already have converted it to csv and to save time and computation would be submitting the .csv.***\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "lMO_nx1i5-4b"
   },
   "outputs": [],
   "source": [
    "\n",
    "# import pandas as pd\n",
    "#df = pd.read_json('AMAZON_FASHION.json', lines=True)\n",
    "#df.to_csv('AMAZON_FASHION.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pJtc4J-3GBFY"
   },
   "source": [
    "## ***3. Manipulating/Cleaning the data:***\n",
    "\n",
    "\n",
    "> The initial .csv file has a lot of noise or junk in it. We need to clean the data properly before using it in our model to ensure the proper reliability and accuracy of the model.\n",
    "\n",
    "***3.1 :*** - Creating the data frame named 'df' and droping the irrelevant and redundant columns (i.e. 'Unnamed: 0', 'reviewTime' , 'reviewerID' , 'asin' ,  'reviewerName' , 'summary' , 'unixReviewTime' , 'vote' ,  'style' , 'image' ) which are not going to be a significant factor/feature for our model development and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "0AR0QxVOj4z9",
    "outputId": "7cd83225-0a1d-4df1-8b26-a122ae7dbf3d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>verified</th>\n",
       "      <th>reviewText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>Exactly what I needed.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>I agree with the other review, the opening is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>Love these... I am going to order another pack...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>too tiny an opening</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>Okay</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   overall  verified                                         reviewText\n",
       "0        5      True                             Exactly what I needed.\n",
       "1        2      True  I agree with the other review, the opening is ...\n",
       "2        4     False  Love these... I am going to order another pack...\n",
       "3        2      True                                too tiny an opening\n",
       "4        3     False                                               Okay"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# load AMAZON_FASHION.csv into a pandas dataframe\n",
    "df = pd.read_csv('AMAZON_FASHION.csv')\n",
    "\n",
    "# print(df.shape)\n",
    "# df.head()\n",
    "\n",
    "#Dropping the unuseful columns.\n",
    "df = df.drop(columns=['Unnamed: 0', 'reviewTime', 'reviewerID', 'asin', 'reviewerName','summary','unixReviewTime','vote','style','image'])\n",
    "# show the first few rows\n",
    "df.head()\n",
    "\n",
    "#df.shape\n",
    "#creating a sample for practicing the code:\n",
    "#df1=df.head(50)\n",
    "#df1.to_csv('AMAZON_FASHION_DEMO.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nyN1M3W-HMaI"
   },
   "source": [
    "***3.2 Mapping:***\n",
    "\n",
    "We already have the ratings class('overall') in numeric format. But for our understanding and readability, let us create a column 'label' by mapping the different numeric ratings with 5 different categorical classes which are as follows: \n",
    "\n",
    "> 1 : 'Very Negative', 2 : 'Negative', 3 : 'Neutral', 4 : 'Positive', 5 : 'Very Positive'\n",
    "\n",
    "Moreover, we are also going to analyse the model having binary output class labels namely, 'Negative' and 'Positive'. So creating a column 'labelbin' with the following mapping to categorical values:\n",
    "\n",
    "> 1 : 'Negative', 2 : 'Negative', 3 : 'Positive', 4 : 'Positive', 5 : 'Positive'\n",
    "\n",
    "But Scikit-learn only deals with numerical values and hence if we were to leave our label values as strings, scikit-learn would do the conversion internally(more specifically, the string labels will be cast to unknown float values).\n",
    "\n",
    "Thus, making a column 'labelbinoverall' by re-mapping the 'Negative' and 'Positive' with the numerical values 0 and 1 respectively:\n",
    "\n",
    "> 'Negative' : 0 , 'Positive\" : 1\n",
    "\n",
    "\n",
    "*   If we left our labels as strings, our model would still be able to produce predictions, but we would run into problems later when calculating performance measures, such as when calculating our precision and recall scores. Therefore, it is best practise to feed our categorical variables into our model as integers in order to prevent unpleasant shocks in the future.\n",
    "\n",
    "*   Also, to get an idea of the size of the dataset we are dealing with, print out number of rows and columns using 'shape'. And getting the glance of the data with 'head()'.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "id": "a_AmxEqr8Vli",
    "outputId": "eb291021-5600-40ba-c456-36c9fa3fe0d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(883636, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>verified</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>label</th>\n",
       "      <th>labelbin</th>\n",
       "      <th>labelbinoverall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>Exactly what I needed.</td>\n",
       "      <td>Very Positive</td>\n",
       "      <td>Positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>I agree with the other review, the opening is ...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>Love these... I am going to order another pack...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>too tiny an opening</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>Okay</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   overall  verified                                         reviewText  \\\n",
       "0        5      True                             Exactly what I needed.   \n",
       "1        2      True  I agree with the other review, the opening is ...   \n",
       "2        4     False  Love these... I am going to order another pack...   \n",
       "3        2      True                                too tiny an opening   \n",
       "4        3     False                                               Okay   \n",
       "\n",
       "           label  labelbin  labelbinoverall  \n",
       "0  Very Positive  Positive                1  \n",
       "1       Negative  Negative                0  \n",
       "2       Positive  Positive                1  \n",
       "3       Negative  Negative                0  \n",
       "4        Neutral  Positive                1  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'] = df.overall.map({0:'Very Negative',1:' Very Negative',2:'Negative',3:'Neutral',4:'Positive',5:'Very Positive'})\n",
    "df['labelbin'] = df.overall.map({0:'Negative',1:'Negative',2:'Negative',3:'Positive',4:'Positive',5:'Positive'})\n",
    "df['labelbinoverall'] = df.labelbin.map({'Negative':0,'Positive':1})\n",
    "\n",
    "print(df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qyr87_LhPt4s"
   },
   "source": [
    "***3.3 Dealing with missing OR null values:***\n",
    "\n",
    "Now, we are going to remove the rows which are null as we would not be able to fill the null places with the mean as we only have columns with string data types which have null values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ogEq8KXcYfYK",
    "outputId": "4a19e7c4-3cf8-401a-c789-f739c6bde27d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overall               0\n",
      "verified              0\n",
      "reviewText         1234\n",
      "label                 0\n",
      "labelbin              0\n",
      "labelbinoverall       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U42lS7CuYi_X",
    "outputId": "91c3a2d3-25c9-42ef-ae1e-b241fdb7d9e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count before dropping:\n",
      " overall            883636\n",
      "verified           883636\n",
      "reviewText         882402\n",
      "label              883636\n",
      "labelbin           883636\n",
      "labelbinoverall    883636\n",
      "dtype: int64 \n",
      "\n",
      "overall            0\n",
      "verified           0\n",
      "reviewText         0\n",
      "label              0\n",
      "labelbin           0\n",
      "labelbinoverall    0\n",
      "dtype: int64 \n",
      "\n",
      "Count after dropping:\n",
      " overall            827526\n",
      "verified           827526\n",
      "reviewText         827526\n",
      "label              827526\n",
      "labelbin           827526\n",
      "labelbinoverall    827526\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Count before dropping:\\n\", df.count(), \"\\n\")\n",
    "\n",
    "# drop any row (axis 0) that has any type of null value\n",
    "df = df.dropna(how='any', axis=0)\n",
    "\n",
    "#dropping rows which are not verified\n",
    "df=df.drop(df[df.verified == False].index,axis=0)\n",
    "#df = df[(df.verified == False)]\n",
    "\n",
    "print(df.isnull().sum(), \"\\n\")\n",
    "\n",
    "print(\"Count after dropping:\\n\", df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "hzeo8Frn0Ia1"
   },
   "outputs": [],
   "source": [
    "#A sample created just for practicing the code\n",
    "# df1=df.head(50)\n",
    "# df1.to_csv('AMAZON_FASHION_DEMO.csv')\n",
    "# df1 = pd.read_csv('AMAZON_FASHION_DEMO.csv')\n",
    "# df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "g7c3mEpbzz6r",
    "outputId": "35aca989-7f23-42ce-cdc8-8bced97ada16"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>verified</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>label</th>\n",
       "      <th>labelbin</th>\n",
       "      <th>labelbinoverall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>Exactly what I needed.</td>\n",
       "      <td>Very Positive</td>\n",
       "      <td>Positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>I agree with the other review, the opening is ...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>too tiny an opening</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>Exactly what I wanted.</td>\n",
       "      <td>Very Positive</td>\n",
       "      <td>Positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>These little plastic backs work great.  No mor...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   overall  verified                                         reviewText  \\\n",
       "0        5      True                             Exactly what I needed.   \n",
       "1        2      True  I agree with the other review, the opening is ...   \n",
       "3        2      True                                too tiny an opening   \n",
       "5        5      True                             Exactly what I wanted.   \n",
       "6        4      True  These little plastic backs work great.  No mor...   \n",
       "\n",
       "           label  labelbin  labelbinoverall  \n",
       "0  Very Positive  Positive                1  \n",
       "1       Negative  Negative                0  \n",
       "3       Negative  Negative                0  \n",
       "5  Very Positive  Positive                1  \n",
       "6       Positive  Positive                1  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VBoB3YGxV58F"
   },
   "source": [
    "***3.4 Plotting a graph for verified ratings and their counts***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 293
    },
    "id": "-WAk1XTrklZe",
    "outputId": "baa7a6cf-596c-4cba-ecdf-f23cf2e0b7cb"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "df['overall'].value_counts()[:].plot(kind='barh')\n",
    "#plt.bar(df1['overall'],len(df1['overall']))\n",
    "\n",
    "# set the y-axis label to \"overall\"\n",
    "plt.ylabel('overall')\n",
    "\n",
    "# rotate the x-ticks to 60 degrees to be visible\n",
    "plt.xticks(rotation=60)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z1tP4pTfXCwC"
   },
   "source": [
    "***The 5-star ratings are way more than any other ratings.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iBJ2GIhGSSPu"
   },
   "source": [
    "## ***4. Bag of words:***\n",
    "\n",
    "Bag of word is a process of extracting features by representing simplified text or data, used in natural language processing and information retrieval. In this model, a text or a document is represented as the bag (multiple set) of its words. So, simply bag of words in sentiment analysis is creating a list of useful words. We have used bag of words approach to extract our feature sets. After preprocessed dataset we used pos tagging to separate different parts of speech and from that we select nouns and adjectives and use those to create a bag of words. Then we run it through a supervised learning and find our results and also the top used words from the review dataset.\n",
    "\n",
    "***What we have here in our data set is a large collection of text data (827,526 rows of data).***\n",
    "\n",
    "***4.1***: Converting all the strings to their lower case. Saving them into a list called 'lowercase_doc'. This can be done by using the lower() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "FfED3hK4xpx9"
   },
   "outputs": [],
   "source": [
    "lowercase_doc = []\n",
    "for i in df['reviewText']:\n",
    "  lowercase_doc.append(i.lower())\n",
    "#print(lowercase_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1N8zWBngdw5b"
   },
   "source": [
    "***4.2 : Removing the punctuations***\n",
    "\n",
    "Removing all punctuation from the lowercaase_doc list's string and saving them into a list called 'nopunct_doc'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "aHEA7yUN9ZeH"
   },
   "outputs": [],
   "source": [
    "nopunct_doc = []\n",
    "\n",
    "for i in lowercase_doc:\n",
    "  nopunct_doc.append(i.translate(str.maketrans('','',string.punctuation)))\n",
    "# print(nopunct_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sWg_NcGWeQPm"
   },
   "source": [
    "***4.3 : Removing Stop Words:***\n",
    "\n",
    "Stop words are the common words which are always present in a sentence just to make it grammatically correct. Thet do not give any meaning to the sentences.\n",
    "\n",
    "Moreover, our dataset is really very large. So it will be profitable to use the stop words removal as it will reduce the data size and would increase the execution speed.\n",
    "\n",
    "Here given is a standard stop word list 'stop_list' with all the common stopwords in english language.\n",
    "\n",
    "Removing the stopwords and the numerical values(if present in the review) and storing the remaining strings in data_ns list.\n",
    "\n",
    "***Tokenization:***\n",
    "It is the process of separating a sequence of strings into individuals such as words, keywords, phrases, symbols and other elements known as tokens. Tokens can be individual words, phrases or even whole sentences. In the process of tokenization, some characters like punctuation marks are discarded. The tokens work as the input for different process like parsing and text mining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "DvToUaif-22s"
   },
   "outputs": [],
   "source": [
    "stop_list = [\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\",\n",
    "             \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\",\n",
    "             \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\",\n",
    "             \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\",\n",
    "             \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\",\n",
    "             \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\",\n",
    "             \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\",\n",
    "             \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\",\n",
    "             \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\",\n",
    "             \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\",\n",
    "             \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\",\n",
    "             \"should\", \"now\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "45QutsDg_LC8"
   },
   "outputs": [],
   "source": [
    "data_ns=[]\n",
    "# j=0\n",
    "for i in nopunct_doc:\n",
    "  \n",
    "  Tkn = re.split('(\\W)', i)\n",
    "  Tkn_ns = [i for i in Tkn if i not in stop_list and i.isalpha()]\n",
    "  data_ns.append(' '.join(Tkn_ns))#, df1['overall'][j]])\n",
    "  # j+=1\n",
    "  # print(df1['overall'](i+1))\n",
    "#print(data_ns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OdMeM4q0jG9J",
    "outputId": "dfe34e86-f679-4fd5-ff38-1e22970ee5bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "827526\n"
     ]
    }
   ],
   "source": [
    "print(len(data_ns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "U2mUiluEfiTe"
   },
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# count_vector = CountVectorizer()\n",
    "\n",
    "# count_vector.fit(data_ns)\n",
    "# count_vector.get_feature_names()\n",
    "# doc_array = count_vector.transform(data_ns).toarray()\n",
    "# frequencies_matrix = pd.DataFrame(doc_array, columns=count_vector.get_feature_names())\n",
    "#frequencies_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c-N_kReFieQg"
   },
   "source": [
    "## ***5. Training and testing sets:***\n",
    "\n",
    "Spliting the dataset into a training and testing set by using the train_test_split method in sklearn. Spliting the data using the following variables:\n",
    "\n",
    "\n",
    "1.   X_train is our training data obtained after preprocessing and forming the list 'data_ns'.\n",
    "\n",
    "2.   y_train is our training data for the 'overall' column\n",
    "\n",
    "3.   X_test is our testing data obtained after preprocessing and forming the list 'data_ns'\n",
    "\n",
    "4.   y_test is our testing data for the 'overall' column \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "TJGKUu8iSlou"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_ns, df['overall'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6GqFXPoWlA6e"
   },
   "source": [
    "## ***A. For 5-Label output:***\n",
    "\n",
    "***6.A. Applying Bag of Words processing to the dataset using CountVectorizer() from sklearn library:***\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "1.   we have to fit our training data (X_train) into CountVectorizer() and return the matrix.\n",
    "2.   we have to transform our testing data (X_test) to return the matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dwzJz_N5S2_B",
    "outputId": "6c50e648-4f9f-4e7f-f7f4-a202fc1e9082"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(620644, 98249)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# Instantiate the CountVectorizer method\n",
    "count_vector = CountVectorizer(analyzer='word',stop_words='english',lowercase=True)\n",
    "\n",
    "# Fit the training data and then return the matrix\n",
    "training_data = count_vector.fit_transform(X_train)\n",
    "print(training_data.shape)\n",
    "\n",
    "# Transform testing data and return the matrix. Note we are not fitting the testing data into the CountVectorizer()\n",
    "testing_data = count_vector.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mfxHmcARlxgK"
   },
   "source": [
    "***7.A. Naive Bayes implementation using scikit-learn:***\n",
    "\n",
    "we will be using the multinomial Naive Bayes implementation. This particular classifier is suitable for classification with discrete features (such as in our case, word counts for text classification). It takes in integer word counts as its input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g3FFO05ET6W1",
    "outputId": "0ff8c1ac-2e47-4b7e-f205-d1d12bd83408"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "naive_bayes = MultinomialNB(alpha=1.0)\n",
    "naive_bayes.fit(training_data,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "xi9QBzyHURhA"
   },
   "outputs": [],
   "source": [
    "predictions = naive_bayes.predict(testing_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KYQa56rCrQpp"
   },
   "source": [
    "so, now that predictions have been made on our test set, lets check the accuracy of our predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BmW4TZQorjFZ"
   },
   "source": [
    "***8.A. Model Evaluation:***\n",
    "\n",
    "Predictions are made on the test set. Now, all we want to do is to evaluate how well our model is doing. There are various mechanisms for doing so:\n",
    "\n",
    "***Accuracy*** : measures how often the classifier makes the correct prediction. Ratio of the number of correct predictions to the total number of predictions (the number of test data points).\n",
    "\n",
    "***Precision*** : what proportion of messages we classified as a class, actually were the same. Ratio of true positives(words classified as a class, and which are actually that specific class) to all positives(all words classified as specific class, irrespective of whether that was the correct classification), in other words it is the ratio of\n",
    "\n",
    "\n",
    "> [True Positives/(True Positives + False Positives)]\n",
    "\n",
    "\n",
    "***Recall*** : tells us what proportion of messages that actually were a specific class were classified by us as the same. It is a ratio of true positives(words classified as a specific class, and which are actually the same) to all the words that were actually that specific class, in other words it is the ratio of\n",
    "\n",
    "\n",
    "\n",
    "> [True Positives/(True Positives + False Negatives)]\n",
    "\n",
    "***F1 score*** : The weighted average of the precision and recall scores. This score can range from 0 to 1, with 1 being the best possible F1 score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tVC0LROetok5"
   },
   "source": [
    "***The 'average=None' parameter of every score function gives the individual score value for every label out of all the 5 output classes in a multi variable classification***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gHUXuFF9Ueiw",
    "outputId": "a3b0bd95-5f34-4310-d113-7d0429123f82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy Score :  0.6453098867953713\n",
      "Precision Score [0.61290674 0.34       0.36935012 0.4090444  0.73639385]\n",
      "Recall Score [0.6767875  0.05127869 0.28805682 0.28597179 0.91141147]\n",
      "F1 Score [0.64326506 0.08911681 0.32367721 0.33661132 0.81460812]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score\n",
    "print(\"accuracy Score : \",accuracy_score(y_test, predictions))\n",
    "print('Precision Score',precision_score(y_test,predictions, average=None))\n",
    "print('Recall Score',recall_score(y_test,predictions, average=None))\n",
    "print('F1 Score',f1_score(y_test,predictions, average=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DjSHD0XbvAva"
   },
   "source": [
    "***Conclusion :*** From the scores it can be observed that the  scores are not uniform across the labels. Moreover, the are not even that significant. One of the reason for this might be the classification into 5 labels. As we increase the number of labels to classify into, every label get less amount of data to train upon.\n",
    "\n",
    "Here, we can observe that for every score category, the extreme scores of 1 : 'Very Negative' and 5 : 'Very Positive' are way more higher than the remaining ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Rqpb4-ywSnv"
   },
   "source": [
    "***So, now let us develop the model with only 2 labels to classify data into: 'Negative' and 'Positive'.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lwVzrXgIwjfV"
   },
   "source": [
    "## ***B. For Binary(2)-Label output***\n",
    "\n",
    "***6.B. Applying Bag of Words processing to the dataset using CountVectorizer() from sklearn library:***\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "1.   we have to fit our training data (X_train_bin) into CountVectorizer() and return the matrix.\n",
    "2.   we have to transform our testing data (X_test_bin) to return the matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "EM2OWzA--bDR"
   },
   "outputs": [],
   "source": [
    "#with 2 labels\n",
    "\n",
    "X_train_bin, X_test_bin, y_train_bin, y_test_bin = train_test_split(data_ns, df['labelbinoverall']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EE2Z3NsR-om-",
    "outputId": "2c1af7a5-ee21-4543-ba6f-8a3b0d222f4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(620644, 98453)\n"
     ]
    }
   ],
   "source": [
    "count_vector_bin = CountVectorizer(analyzer='word',stop_words='english',lowercase=True)\n",
    "\n",
    "# Fit the training data and then return the matrix\n",
    "training_data_bin = count_vector_bin.fit_transform(X_train_bin)\n",
    "print(training_data_bin.shape)\n",
    "\n",
    "# Transform testing data and return the matrix. Note we are not fitting the testing data into the CountVectorizer()\n",
    "testing_data_bin = count_vector_bin.transform(X_test_bin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nu414qJ7JKcz"
   },
   "source": [
    "***7.B. Naive Bayes implementation using scikit-learn (for binary-Label):***\n",
    "\n",
    "we will be using the multinomial Naive Bayes implementation. This particular classifier is suitable for classification with discrete features (such as in our case, word counts for text classification). It takes in integer word counts as its input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "njLnXiWF-rWl",
    "outputId": "506fbcec-ef67-45ab-9a52-1666372e61b8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive_bayes_bin = MultinomialNB(alpha=1.0)\n",
    "naive_bayes_bin.fit(training_data_bin,y_train_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "JAjgFMRp-u6w"
   },
   "outputs": [],
   "source": [
    "predictions_bin = naive_bayes_bin.predict(testing_data_bin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QIK_wM9nJcAJ"
   },
   "source": [
    "So, now that predictions have been made on our test set, lets check the accuracy of our predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HRKIcEUCJhpW"
   },
   "source": [
    "**8.B. Model Evaluation (for binary-Label):**\n",
    "\n",
    "Predictions are made on the test set. Now, all we want to do is to evaluate how well our model is doing. There are various mechanisms for doing so:\n",
    "\n",
    "**Accuracy** : measures how often the classifier makes the correct prediction. Ratio of the number of correct predictions to the total number of predictions (the number of test data points).\n",
    "\n",
    "**Precision** : what proportion of messages we classified as a True, actually were True. Ratio of true positives(words classified as True, and which are actually True) to all positives(all words classified as True, irrespective of whether that was the correct classification), in other words it is the ratio of\n",
    "\n",
    "\n",
    "\n",
    "> [True Positives/(True Positives + False Positives)]\n",
    "\n",
    "**Recall** : tells us what proportion of messages that actually were True were classified by us as True. It is a ratio of true positives(words classified as True, and which are actually True) to all the words that were actually True, in other words it is the ratio of\n",
    "\n",
    "\n",
    "\n",
    "> [True Positives/(True Positives + False Negatives)]\n",
    "\n",
    "**F1 score** : The weighted average of the precision and recall scores. This score can range from 0 to 1, with 1 being the best possible F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vu0fF2X2-04V",
    "outputId": "b6f369f4-6c32-410e-bcc9-3b5d38b8caef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score :  0.887312574317727\n",
      "Precision Score [0.74565972 0.91572173]\n",
      "Recall Score [0.63956519 0.94723541]\n",
      "F1 Score [0.68854956 0.93121203]\n",
      "Average Accuracy Score 0.887312574317727\n",
      "Average Precision Score 0.9157217302491847\n",
      "Average Recall Score 0.9472354117018531\n",
      "Average F1 Score 0.9312120291167888\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy Score : \",accuracy_score(y_test_bin, predictions_bin))\n",
    "print('Precision Score',precision_score(y_test_bin,predictions_bin, average=None))\n",
    "print('Recall Score',recall_score(y_test_bin,predictions_bin, average=None))\n",
    "print('F1 Score',f1_score(y_test_bin,predictions_bin, average=None))\n",
    "print('Average Accuracy Score',accuracy_score(y_test_bin, predictions_bin))\n",
    "print('Average Precision Score',precision_score(y_test_bin,predictions_bin))\n",
    "print('Average Recall Score',recall_score(y_test_bin,predictions_bin))\n",
    "print('Average F1 Score',f1_score(y_test_bin,predictions_bin))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wqqb_YRULBxz"
   },
   "source": [
    "***Conclusion:*** Every score has increased significantly as soon as the number of labels are decreased. The average accuracy is around 89% and the average precision is around 92% for binary label analysis.\n",
    "\n",
    "Here, we can observe that for every score category, the score of the 1:'Positive' label is much higher than the 0:'Negative'. This might be because 1:'Positive' review data were way more than other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ZuR1iunLDDE"
   },
   "source": [
    "***So, now let us develop the model having 2 labels -using lemmatization- to classify data into: 'Negative' and 'Positive'.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UZfDN6MuLqYy"
   },
   "source": [
    "***Lemmatization is the process of grouping together the different inflected forms of a word so they can be analyzed as a single item. Lemmatization is similar to stemming but it brings context to the words. So it links words with similar meanings to one word. Thus lemmatization can further reduce the corpus size and enhance computation speed.***\n",
    "\n",
    "*   Lemmatizing the nopunct_doc list and removing stop words to form data_ns_lem using nltk's WordNetLemmatizer()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P-VvprPGDVc2",
    "outputId": "685ac122-fe58-4765-d731-4c03415140fa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/amanshah/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/amanshah/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "data_ns_lem,Tkn_ns_lem=[],[]\n",
    "#print(len(Tkn_ns_lem))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "  \n",
    "#print(\"wanted :\", lemmatizer.lemmatize(\"wanted\",pos='a'))\n",
    "#print(\"wanted :\", lemmatizer.lemmatize(\"wanted\",pos='n'))\n",
    "#j=0\n",
    "for i in nopunct_doc:\n",
    "  \n",
    "  Tkn_lem = re.split('(\\W)', i)\n",
    "  Tkn_ns_lem = [lemmatizer.lemmatize(i) for i in Tkn_lem if i not in stop_list and i.isalpha()]\n",
    "  # if j<=9:\n",
    "  #   print(Tkn_ns_lem)\n",
    "  #   j+=1\n",
    "  data_ns_lem.append(' '.join(Tkn_ns_lem))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mb72jPSPgB6E",
    "outputId": "4c7a403f-f735-4340-c76c-c0f3cd30b8d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "827526\n"
     ]
    }
   ],
   "source": [
    "print(len(data_ns_lem))\n",
    "#mylist = [\"a\", \"b\", \"a\", \"c\", \"c\"]\n",
    "#data_ns_lem = list(dict.fromkeys(data_ns_lem))\n",
    "#print(len(data_ns_lem))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vEUEPpN6L_8x"
   },
   "source": [
    "## ***C. For Binary(2)-Label output  -WITH LEMMATIZATION-*** \n",
    "\n",
    "***6.C. Applying Bag of Words processing to the dataset using CountVectorizer() from sklearn library:***\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "1.   we have to fit our training data (X_train_bin_lem) into CountVectorizer() and return the matrix.\n",
    "2.   we have to transform our testing data (X_test_bin_lem) to return the matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "-9OmyDu6jnYE"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_bin_lem, X_test_bin_lem, y_train_bin_lem, y_test_bin_lem = train_test_split(data_ns_lem, df['labelbinoverall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MExU_psfkCqO",
    "outputId": "26c26f5b-b0e7-4a61-b41d-b52149855368"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(620644, 92806)\n"
     ]
    }
   ],
   "source": [
    "count_vector_bin_lem = CountVectorizer(analyzer='word',stop_words='english',lowercase=True)\n",
    "\n",
    "# Fit the training data and then return the matrix\n",
    "training_data_bin_lem = count_vector_bin_lem.fit_transform(X_train_bin_lem)\n",
    "print(training_data_bin_lem.shape)\n",
    "\n",
    "# Transform testing data and return the matrix. Note we are not fitting the testing data into the CountVectorizer()\n",
    "testing_data_bin_lem = count_vector_bin_lem.transform(X_test_bin_lem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3m5QABOUM29t"
   },
   "source": [
    "***7.C. Naive Bayes implementation using scikit-learn (for 2-Label -WITH LEMMATIZATION-):***\n",
    "\n",
    "The capability of Naive Bayes to handle an exceptionally large number of features is one of its key benefits over other classification methods. Since there are thousands of different words, they are all treated as features in our situation. Additionally, it functions well even when irrelevant features are present and is largely unaffected by them. The other major advantage it has is its relative simplicity. Naive Bayes' works well right out of the box and tuning it's parameters is rarely ever necessary, except usually in cases where the distribution of the data is known. It rarely ever overfits the data. Another important advantage is that its model training and prediction times are very fast for the amount of data it can handle.\n",
    "\n",
    "we will be using the multinomial Naive Bayes implementation. This particular classifier is suitable for classification with discrete features (such as in our case, word counts for text classification). It takes in integer word counts as its input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M29_QKurkc_s",
    "outputId": "0d9ed820-0dfd-4c37-8d63-3768fc9a629d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-6 {color: black;background-color: white;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" checked><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive_bayes_bin_lem = MultinomialNB(alpha=1.0)\n",
    "naive_bayes_bin_lem.fit(training_data_bin_lem,y_train_bin_lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "2vO-IghLkvO2"
   },
   "outputs": [],
   "source": [
    "predictions_bin_lem = naive_bayes_bin_lem.predict(testing_data_bin_lem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jq8qmGwjOhtw"
   },
   "source": [
    "So, now that predictions ***after lemmatization*** have been made on our test set, lets check the accuracy of our predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DtKTYvdtOvNy"
   },
   "source": [
    "***8.C. Model Evaluation (for 2-Label -WITH LEMMATIZATION-):***\n",
    "\n",
    "Predictions are made on the test set. Now, all we want to do is to evaluate how well our model is doing. There are various mechanisms for doing so:\n",
    "\n",
    "***Accuracy*** : measures how often the classifier makes the correct prediction. Ratio of the number of correct predictions to the total number of predictions (the number of test data points).\n",
    "\n",
    "***Precision*** : what proportion of messages we classified as a True, actually were True. Ratio of true positives(words classified as True, and which are actually True) to all positives(all words classified as True, irrespective of whether that was the correct classification), in other words it is the ratio of\n",
    "\n",
    "[True Positives/(True Positives + False Positives)]\n",
    "\n",
    "***Recall*** : tells us what proportion of messages that actually were True were classified by us as True. It is a ratio of true positives(words classified as True, and which are actually True) to all the words that were actually True, in other words it is the ratio of\n",
    "\n",
    "[True Positives/(True Positives + False Negatives)]\n",
    "\n",
    "***F1 score*** : The weighted average of the precision and recall scores. This score can range from 0 to 1, with 1 being the best possible F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "65AtcvBck_x-",
    "outputId": "efa16843-55a1-488f-96d8-0874f385fc23"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score 0.8862588335379589\n",
      "Precision Score [0.74702459 0.91383154]\n",
      "Recall Score [0.63192005 0.94802831]\n",
      "F1 Score [0.68466827 0.93061588]\n",
      "\n",
      "\n",
      "average Accuracy Score 0.8862588335379589\n",
      "average Precision Score 0.9138315429828879\n",
      "average Recall Score 0.9480283077810352\n",
      "average F1 Score 0.9306158795309325\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy Score',accuracy_score(y_test_bin_lem, predictions_bin_lem))\n",
    "print('Precision Score',precision_score(y_test_bin_lem,predictions_bin_lem,average=None))\n",
    "print('Recall Score',recall_score(y_test_bin_lem,predictions_bin_lem,average=None))\n",
    "print('F1 Score',f1_score(y_test_bin_lem,predictions_bin_lem,average=None))\n",
    "print('\\n')\n",
    "print('average Accuracy Score',accuracy_score(y_test_bin_lem, predictions_bin_lem))\n",
    "print('average Precision Score',precision_score(y_test_bin_lem,predictions_bin_lem))\n",
    "print('average Recall Score',recall_score(y_test_bin_lem,predictions_bin_lem))\n",
    "print('average F1 Score',f1_score(y_test_bin_lem,predictions_bin_lem))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NeJe_ZuoO_OW"
   },
   "source": [
    "***Conclusion:*** Every score has increased a bit except Precision score when the data is lemmatized. Lemmatization has shown a positive impact on the model. \n",
    "\n",
    "Here, we can observe that for every score category, the score of the 1 : 'Positive' label is much higher than the 0 : 'Negative'. This might be because 1 : 'Positive' review data were way more than other.\n",
    "\n",
    "***Thus, the binary(2)-Label model with lemmatization is more efficient compared to others.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xF1YaSU1mow6"
   },
   "source": [
    "### ***References:*** \n",
    "Stackoverflow, Tutorials, geeksforgeeks.org\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
